📊 ExAct Benchmark
ExAct is a video-language benchmark designed to evaluate expert-level understanding of skilled human actions across diverse physical domains.

📁 Dataset
The dataset is available on Hugging Face:
🔗 https://huggingface.co/datasets/Alexhimself/ExAct

🧪 Evaluation Code
This repository contains the evaluation code used to assess the performance of 8 vision-language models (VLMs) on ExAct:

GPT-4o~\cite{hurst2024gpt}

Gemini 1.5 Pro~\cite{team2023gemini}

LLaVA-Video~\cite{zhang2024video}

LLaVA-OneVision~\cite{li2024llava}

Qwen2.5-VL~\cite{bai2025qwen2}

VideoLLaMA~\cite{zhang2025videollama}

InternVL2.5~\cite{chen2024internvl}

PerceptionLM~\cite{cho2025perceptionlm}

We provide standardized prompts, model-specific wrappers, and evaluation scripts for consistent comparison across all models.
